{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9949ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainfile = \"\"\"\\\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import configparser\n",
    "import sys\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, array, ArrayType, DateType, DecimalType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import concat, lit, col, udf\n",
    "from delta import * \n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "logger = logging.getLogger(__name__) # __name__=docai\n",
    "logger.info(\"This is an INFO message on the root logger.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Start Spark Session\n",
    "    \n",
    "    table_name = \"delta-table\"\n",
    "    container_name = \"update-me\"\n",
    "    storage_account_name = \"update-me\"\n",
    "    account_access_key = \"update-me\"\n",
    "\n",
    "    try:\n",
    "        builder = SparkSession \\\n",
    "                    .builder \\\n",
    "                    .appName(\"DocumentAI\") \\\n",
    "                    .master(\"local\") \\\n",
    "                    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.1.0\") \\\n",
    "                    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure-datalake:3.3.1\") \\\n",
    "                    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                    .config(f\"spark.hadoop.fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"SharedKey\") \\\n",
    "                    .config(f\"spark.hadoop.fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", f\"{account_access_key}\")\n",
    "\n",
    "        logger.info(SparkConf().getAll())\n",
    "        spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "        spark.sparkContext.setLogLevel('INFO')\n",
    "    \n",
    "    except Exception as error:\n",
    "        logger.info(f\"Exception occurred {error}\")\n",
    "        logger.info(\"Spark builder connection prompted out due to : %s\", error)\n",
    "        logger.info(traceback.format_exc())\n",
    "\n",
    "    # Create a data schema\n",
    "    schema = StructType([\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"dates\", StringType(), True),\n",
    "            StructField(\"population\", IntegerType(), True)])\n",
    "\n",
    "\n",
    "    dates = [\"1991-02-25\",\"1998-05-10\", \"1993/03/15\", \"1992/07/17\", \"1992-05-23\", \"2022-06-23\"]\n",
    "    cities = ['Caracas', 'Ccs', 'SÃ£o Paulo', 'Madrid', \"San Francisco\", \"New Hampshire\"]\n",
    "    population = [37800000, 19795791, 12341418, 6489162, 8483993, 76234589]\n",
    "\n",
    "    # Dataframe:\n",
    "    data = spark.createDataFrame(list(zip(cities, dates, population)), schema=schema)\n",
    "\n",
    "    # write to delta-lake\n",
    "    # data.write.format(\"delta\").save(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{table_name}\")\n",
    "    # logger.info(\"Successfully written\")\n",
    "    # data.show(truncate=False)\n",
    "\n",
    "    # Append the new-data to delta-table\n",
    "    data.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\")  \\\n",
    "        .save(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{table_name}\")\n",
    "\n",
    "\n",
    "    # Read from delta-table\n",
    "    new_df = spark.read.format(\"delta\").load(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{table_name}\")\n",
    "    new_df.show()\n",
    "\n",
    "    spark.stop()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f5fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write mainfile\n",
    "\n",
    "mainpy_path = './main.py'\n",
    "with open(mainpy_path,'w') as f:\n",
    "    f.write(mainfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkjob = \"\"\"\\\n",
    "    from __future__ import print_function\n",
    "import pyspark\n",
    "import traceback\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, LongType,  DateType\n",
    "from pyspark.sql.functions import *\n",
    "from delta import * \n",
    "from delta.tables import DeltaTable\n",
    "import shutil\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "logger = logging.getLogger(__name__) # __name__=docai\n",
    "logger.info(\"This is an INFO message on the root logger.\")\n",
    "\n",
    "\n",
    "logger.info(\"Get configuration files to create a connection to Azure Storage Gen2\")\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg.template', encoding='utf-8-sig')\n",
    "container_name       =  config['AZURE']['CONTAINER_NAME']\n",
    "storage_account_name =  config['AZURE']['STORAGE_ACCOUNT_NAME']\n",
    "account_access_key   =  config['AZURE']['ACCOUNT_ACCESS_KEY']\n",
    "\n",
    "logger.info(\"Get storage name for table and raw data\")\n",
    "table_name             = config['STORAGE']['TABLE_NAME']\n",
    "Azure_10k_filings_data = config['STORAGE']['AZURE_10K_FILINGS_DATA']\n",
    "\n",
    "logger.info(\"Delta table and storage data url\")\n",
    "AZURE_10K_CSV_DATA = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{Azure_10k_filings_data}\"\n",
    "DELTA_TABLE        = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{table_name}\"\n",
    "    \n",
    "    \n",
    "# load data from Azure Gen2\n",
    "def session(spark):\n",
    "\n",
    "    logger.info(\"predefined schema of StrucType\")\n",
    "    docai_schema =  StructType(\n",
    "                        [StructField(\"cik_number\", LongType(), True),\n",
    "                        StructField(\"company_name\", StringType(), True),\n",
    "                        StructField(\"form_id\", StringType(), True),\n",
    "                        StructField(\"date\", DateType(), True),\n",
    "                        StructField(\"file_url\", StringType(), True)\n",
    "                        ])\n",
    "    \n",
    "    logger.info(\"read csv dataframe from Azure\")\n",
    "    logger.info(f\"reading {Azure_10k_filings_data} filings data from azure storage : {storage_account_name}\")\n",
    "    df_filings = spark.read \\\n",
    "                      .format(\"csv\") \\\n",
    "                      .option(\"header\", \"true\") \\\n",
    "                      .option(\"inferSchema\", \"true\") \\\n",
    "                      .option(\"nullValue\", \"null\") \\\n",
    "                      .load(AZURE_10K_CSV_DATA)\n",
    "    \n",
    "    # If there is a need to specify the schema\n",
    "    df_filings = spark.read \\\n",
    "                      .format(\"csv\") \\\n",
    "                      .option(\"header\", \"true\") \\\n",
    "                      .schema(docai_schema) \\\n",
    "                      .option(\"nullValue\", \"null\") \\\n",
    "                      .load(AZURE_10K_CSV_DATA)\n",
    "                      \n",
    "                      \n",
    "    df_filings.show(15)\n",
    "    \n",
    "\n",
    "    # Clear any previous runs\n",
    "    logger.info(\"clearing any previous runs\")\n",
    "    shutil.rmtree(DELTA_TABLE, ignore_errors=True)\n",
    "                                         \n",
    "    logger.info(\"Atomically append new data to an existing Delta table\")\n",
    "    logger.info(f\"Updating/Appending data to the delta table : {table_name}\")\n",
    "    df_filings.write \\\n",
    "              .format(\"delta\") \\\n",
    "              .mode(\"append\")  \\\n",
    "              .save(DELTA_TABLE)\n",
    "    \n",
    "    logger.info(f\"Data stored/appended to the Delta table {table_name}\")\n",
    "\n",
    "\n",
    "def readTable(spark):\n",
    "    \n",
    "    logger.info(\"sample data from the delta-table\")\n",
    "    table_data = spark.read \\\n",
    "                      .format(\"delta\") \\\n",
    "                      .option(\"header\", \"true\") \\\n",
    "                      .option(\"inferSchema\", \"true\") \\\n",
    "                      .load(DELTA_TABLE)\n",
    "                      \n",
    "    return table_data.show(10)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    try:\n",
    "        builder = SparkSession.builder \\\n",
    "                .appName(\"Documentai\") \\\n",
    "                .master(\"local[*]\") \\\n",
    "                .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.1.0\") \\\n",
    "                .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure-datalake:3.3.1\") \\\n",
    "                .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                .config(f\"spark.hadoop.fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"SharedKey\")\\\n",
    "                .config(f\"spark.hadoop.fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", f\"{account_access_key}\") \n",
    "\n",
    "        logger.info(SparkConf().getAll())\n",
    "        spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "        spark.sparkContext.setLogLevel('INFO')\n",
    "        \n",
    "    except Exception as error:\n",
    "        logger.info(f\"Exception occurred {error}\")\n",
    "        logger.info(\"Spark builder connection prompted out due to : %s\", error)\n",
    "        logger.info(traceback.format_exc())\n",
    "        \n",
    "    logger.info(\"instantiating the session to read data from Azure and write to delta-lake\")\n",
    "    session(spark)\n",
    "    \n",
    "    logger.info(\"Read and display few rows of data from delta-table\")\n",
    "    readTable(spark)\n",
    "    \n",
    "    logger.info(\"Stopping Spark session...\")\n",
    "    spark.stop()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec641796",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_job_path = './sparkjob.py'\n",
    "with open(spark_job_path,'w') as f:\n",
    "    f.write(sparkjob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b998d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile = \"\"\"\\\n",
    "FROM gcr.io/datamechanics/spark:3.2.0-hadoop-3.3.1-java-11-scala-2.12-python-3.8-dm16\n",
    "\n",
    "RUN python -m venv /opt/spark-env\n",
    "\n",
    "USER root\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "RUN . /opt/spark-env/bin/activate && pip install --upgrade pip\n",
    "\n",
    "COPY requirements.txt /app/\n",
    "RUN . /opt/spark-env/bin/activate && pip install -r requirements.txt\n",
    "\n",
    "COPY main.py /app\n",
    "COPY sparkjob.py /app\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a083fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_path = './Dockerfile'\n",
    "with open(docker_path,'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29b6313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = \"\"\"\\\n",
    "pyspark \n",
    "delta-spark\n",
    "click\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c03f8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = './requirements.txt'\n",
    "with open(text_path,'w') as f:\n",
    "    f.write(requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2973d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build docker image\n",
    "\n",
    "!docker build -t sparkrun ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b07937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view the built docker images\n",
    "\n",
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f361c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the credentials to connect to Azure\n",
    "\n",
    "resourceGroupName = '<rg name>' \n",
    "location ='<rg location>'\n",
    "acrName = '<acr name>'\n",
    "tenant_id = '<tenant_id>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $resourceGroupName\n",
    "!echo $location\n",
    "!echo $acrName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c069055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Azure portal\n",
    "\n",
    "!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e10f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to ACR\n",
    "\n",
    "!az acr login --name $acrName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8338d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag the docker image to be pushed to ACR\n",
    "\n",
    "!docker tag sparkrun $acrName\".azurecr.io/sparkrun:v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bbf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the docker image to ACR\n",
    "\n",
    "!docker push $acrName\".azurecr.io/sparkrun:v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de364934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the docker image in the ACR repository\n",
    "\n",
    "!az acr repository list --name $acrName --output table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
