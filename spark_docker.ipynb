{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9949ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainfile = \"\"\"\\\n",
    "import pyspark \n",
    "import sys \n",
    "import shutil\n",
    "from pyspark import SparkContext, SparkConf  \n",
    "from pyspark.sql import SparkSession  \n",
    "from delta import *  \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "import click\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "logger = logging.getLogger(__name__) # __name__=docai\n",
    "logger.info(\"This is an INFO message on the root logger.\")\n",
    "\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg', encoding='utf-8-sig')\n",
    "\n",
    "container_name       =  config['AZURE']['CONTAINER_NAME']\n",
    "storage_account_name =  config['AZURE']['STORAGE_ACCOUNT_NAME']\n",
    "account_access_key   =  config['AZURE']['ACCOUNT_ACCESS_KEY']\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    try:   # Start Spark Session\n",
    "\n",
    "        builder = SparkSession.builder \\\n",
    "            .appName(\"documentai\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.1.0\") \\\n",
    "            .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure-datalake:3.1.1\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .config(f\"spark.hadoop.fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"SharedKey\")\\\n",
    "            .config(f\"spark.hadoop.fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",f\"{account_access_key}\")\n",
    "\n",
    "        spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "    except Exception as error:\n",
    "\n",
    "        logger.info(\"Spark builder connection prompted out due to : %s\", error)\n",
    "\n",
    "\n",
    "        docai_schema =  StructType([StructField(\"S.no\", StringType()),\n",
    "                        StructField(\"Item 1\", StringType()),\n",
    "                        StructField(\"Item 1A\", StringType()),\n",
    "                        StructField(\"Item 2\", StringType()),\n",
    "                        StructField(\"Item 5\", StringType()),\n",
    "                        StructField(\"Item 6\", StringType()),\n",
    "                        StructField(\"Item 7\", StringType()),  \n",
    "                        StructField(\"Item 7a\", StringType()),\n",
    "                        StructField(\"Item 8\", StringType()),\n",
    "                        StructField(\"DIRECTORS, EXECUTIVE OFFICERS AND CORPORATE GOVERNANCE (References only)\", StringType()),\n",
    "                        StructField(\"CERTAIN RELATIONSHIPS AND RELATED TRANSACTIONS, AND DIRECTOR INDEPENDENCE (References only)\", StringType()),\n",
    "                        StructField(\"Item 15\", StringType())\n",
    "                        ])\n",
    "\n",
    "\n",
    "        @click.command()\n",
    "        @click.option('--raw_data_bucket', prompt='Azure raw bucket-name', help='Name of Raw data Bucket.')\n",
    "        @click.option('--start_date', prompt='Enter Start date', help='The start date to get data.')\n",
    "        @click.option('--end_date', prompt='Enter end date', help='The end date to get data.')\n",
    "        @click.option('--delta_table', prompt='Enter delta lake table name', help='The delta table name.')\n",
    "\n",
    "\n",
    "        def session(raw_data_bucket, start_date, end_date, delta_table):\n",
    "        \n",
    "            click.echo(f\"This is the Azure bucket to be used {raw_data_bucket}!\")\n",
    "            click.echo(f\"This is the start date {start_date}!\")\n",
    "            click.echo(f\"This is the end date {end_date}!\")\n",
    "            click.echo(f\"This is the delta lake table {delta_table}!\")\n",
    "\n",
    "            \n",
    "            AZURE_RAW_DATA   = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{raw_data_bucket}/*.txt\"\n",
    "            TABLE_PATH = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{delta_table}\"\n",
    "\n",
    "\n",
    "            # clear previous run's delta-table\n",
    "            shutil.rmtree(\"AZURE_RAW_DATA\", ignore_errors=True)\n",
    "\n",
    "            # Read file from Azure blob storage\n",
    "            df_docai = spark \\\n",
    "                        .read \\\n",
    "                        .option(\"inferSchema\", \"true\") \\\n",
    "                        .option(\"header\", \"true\") \\\n",
    "                        .text(AZURE_RAW_DATA, \n",
    "                                    lineSep=\",\", \n",
    "                                        wholetext=True,\n",
    "                                            header=False, \n",
    "                                                schema=docai_schema)\n",
    "            df_docai.show(5)\n",
    "\n",
    "            # Convert to Delta\n",
    "            deltatable = DeltaTable.convertToDelta(spark, \"parquet.`docai-table`\")\n",
    "\n",
    "\n",
    "            # write to delta table\n",
    "            deltatable.write.format(\"delta\") \\\n",
    "                            .mode(\"overwrite\") \\\n",
    "                            .save(TABLE_PATH)\n",
    "\n",
    "            # Read delta file\n",
    "            df_docai = spark.read.format(\"delta\").load(TABLE_PATH,\n",
    "                                            header=True, \n",
    "                                            schema=docai_schema)\n",
    "            df_docai.show(10)\n",
    "\n",
    "\n",
    "            # Read table with DeltaTable\n",
    "            deltaTable = DeltaTable.forPath(spark, \"\")\n",
    "            deltaTable.toDF().show()\n",
    "\n",
    "\n",
    "    spark.stop()\n",
    "session()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f5fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write mainfile\n",
    "\n",
    "mainpy_path = './main.py'\n",
    "with open(mainpy_path,'w') as f:\n",
    "    f.write(mainfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b998d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "dockerfile = \"\"\"\\\n",
    "FROM gcr.io/spark-operator/spark-py:v3.1.1\n",
    "\n",
    "# switch to user root so we can add additional jars and configuration files.\n",
    "# USER root:root\n",
    "USER root\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "RUN apt-get install python3-pip\n",
    "\n",
    "COPY requirements.txt /app/\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY spark_job.py  /app\n",
    "\n",
    "ENTRYPOINT [ \"/opt/entrypoint.sh\" ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a083fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_path = './Dockerfile'\n",
    "with open(docker_path,'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29b6313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = \"\"\"\\\n",
    "pyspark\n",
    "delta-spark\n",
    "click\n",
    "# wget\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c03f8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = './requirements.txt'\n",
    "with open(text_path,'w') as f:\n",
    "    f.write(requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61e2973d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "error during connect: This error may indicate that the docker daemon is not running.: Post \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&shmsize=0&t=sparkrun&target=&ulimits=null&version=1\": open //./pipe/docker_engine: The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "# Build docker image\n",
    "\n",
    "!docker build -t sparkrun ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b07937",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f361c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "resourceGroupName = 'BMA-nlp-infra' \n",
    "location ='canadacentral'\n",
    "acrName = 'infranlpacr'\n",
    "tenant_id = 'XXXX'\n",
    "subscription_id = 'XXXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $resourceGroupName\n",
    "!echo $location\n",
    "!echo $acrNameb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c069055",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login --tenant tenant_id --subscription subscription_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e10f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az acr login --name $acrName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8338d0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag sparkrun $acrName\".azurecr.io/sparkrun:v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bbf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push $acrName\".azurecr.io/sparkrun:v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de364934",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az acr repository list --name $acrName --output table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
